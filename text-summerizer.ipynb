{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Text Summarization\n",
    " ===========================\n",
    "###  Author : Dur e Yashfeen \n",
    "### Date : 10- Feb- 2025\n",
    "### Objective: Create a system that summarizes lengthy articles, blogs, or news into concise  summaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ“– The Story of Smart Summarization âœ¨ğŸ“œ\n",
    "\n",
    "In a fast-paced digital world ğŸŒ, people struggle to keep up with endless articles, research papers, and reports ğŸ“„. Meet Sarah, a student ğŸ“š who often felt overwhelmed by lengthy academic papers ğŸ˜µ. She wished for a magical tool to extract key points quickly! ğŸƒâ€â™€ï¸ğŸ’¡\n",
    "\n",
    "One day, Sarah discovered an AI-powered text summarizer ğŸ§ ğŸ¤–. With just a click, the tool analyzed massive texts and generated concise, meaningful summaries âœï¸ğŸ”. Now, Sarah could focus on understanding concepts without spending hours reading! ğŸ•’ğŸ‰\n",
    "\n",
    "This AI summarizer changed Sarahâ€™s life, making studying efficient and enjoyable! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Steps for Text Summarization ğŸ“\n",
    "1. **Import Necessary Libraries** â€“ Load essential tools for text processing.\n",
    "2. **Load and Process Text** â€“ Tokenize the text into sentences.\n",
    "3. **Extract Important Sentences** â€“ Use TF-IDF and dimensionality reduction to select key sentences.\n",
    "4. **Generate Summary** â€“ Extract meaningful information and display the summary.\n",
    "5. **Conclusion** â€“ Understanding how AI simplifies summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import pipeline, BartForConditionalGeneration, BartTokenizer\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from rouge import Rouge\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "def load_data(train_path, test_path, val_path):\n",
    "    train_data = pd.read_csv(\"./datasets/train.csv\")\n",
    "    test_data = pd.read_csv(\"./datasets/train.csv\")\n",
    "    val_data = pd.read_csv(\"./datasets/validation.csv\")\n",
    "    return train_data, test_data, val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\DUR E\n",
      "[nltk_data]     YASHFEEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocess textual data for summarization\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English NLP model from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text for summarization\n",
    "text = \"\"\"Your input article or text goes here. The model will preprocess, extract important sentences, \n",
    "and generate an abstractive summary based on context.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 1ï¸âƒ£ Preprocessing Function ###\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your input article text go the model preprocess extract important sentence generate abstractive summary base context\n"
     ]
    }
   ],
   "source": [
    "# Tokenization, removing stopwords, and lemmatization\n",
    "doc = nlp(text)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\t\n",
    "filtered_tokens = [token.lemma_ for token in doc if token.text not in stop_words and not token.is_punct]\n",
    "\t\n",
    "filtered_text = \" \".join(filtered_tokens)\n",
    "print(filtered_text)\n",
    "processed_text = preprocess_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2ï¸âƒ£ Extractive Summarization (TextRank + Frequency-based) ###\n",
    "def extractive_summary(text, num_sentences=3):\n",
    "    doc = nlp(text)\n",
    "    sentence_scores = {}\n",
    "    \n",
    "    word_freq = Counter([token.text for token in doc if token.is_alpha])\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        for word in sent:\n",
    "            if word.text in word_freq:\n",
    "                sentence_scores[sent] = sentence_scores.get(sent, 0) + word_freq[word.text]\n",
    "    \n",
    "    summary_sentences = nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    return \" \".join([sent.text for sent in summary_sentences])\n",
    "\n",
    "extractive_result = extractive_summary(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 3ï¸âƒ£ Abstractive Summarization (BART Transformer) ###\n",
    "# Load pre-trained model\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def abstractive_summary(text, max_length=150):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=max_length, min_length=40, length_penalty=2.0, num_beams=4)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "abstractive_result = abstractive_summary(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ Extractive Summary:\n",
      "Your input article or text goes here The model will preprocess extract important sentences and generate an abstractive summary based on context\n",
      "\n",
      "ğŸ“Œ Abstractive Summary:\n",
      "summarize: Your input article or text goes here. The model will preprocess extract important sentences and generate an abstractive summary based on context. It will generate a summary of an article based on its context.\n",
      "\n",
      "ğŸš€ ROUGE Evaluation Scores:\n",
      "[{'rouge-1': {'r': 1.0, 'p': 0.8148148148148148, 'f': 0.897959178725531}, 'rouge-2': {'r': 1.0, 'p': 0.6363636363636364, 'f': 0.7777777730246914}, 'rouge-l': {'r': 1.0, 'p': 0.8148148148148148, 'f': 0.897959178725531}}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### 4ï¸âƒ£ Evaluate Summaries (ROUGE Score) ###\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(abstractive_result, extractive_result)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nğŸ“Œ Extractive Summary:\")\n",
    "print(extractive_result)\n",
    "\n",
    "print(\"\\nğŸ“Œ Abstractive Summary:\")\n",
    "print(abstractive_result)\n",
    "\n",
    "print(\"\\nğŸš€ ROUGE Evaluation Scores:\")\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Conclusion ğŸ¯âœ…\n",
    "\n",
    "With AI-driven text summarization, reading long documents becomes effortless! ğŸš€ Now, students like Sarah and professionals can grasp essential information quickly and stay ahead in their fields! ğŸ“šâœ¨\n",
    "\n",
    "---\n",
    "\n",
    "The End ğŸ¬ğŸ‰\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
